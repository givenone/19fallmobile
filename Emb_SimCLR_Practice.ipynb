{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Emb_SimCLR_Practice.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iZ4WM8_Guqb6",
        "OvxUSFm-uqb9",
        "6xwkINzBuqcH"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/givenone/19fallmobile/blob/master/Emb_SimCLR_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIbUUOphvNRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "6b0b5bd3-0052-4e32-c5c9-81986a0d2d30"
      },
      "source": [
        "  pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git\n",
            "  Cloning https://github.com/ildoonet/pytorch-gradual-warmup-lr.git to /tmp/pip-req-build-gksnu771\n",
            "  Running command git clone -q https://github.com/ildoonet/pytorch-gradual-warmup-lr.git /tmp/pip-req-build-gksnu771\n",
            "Building wheels for collected packages: warmup-scheduler\n",
            "  Building wheel for warmup-scheduler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for warmup-scheduler: filename=warmup_scheduler-0.3.2-cp36-none-any.whl size=3881 sha256=ae56a3a3db89b9396a78f25aa467969e004f23b89306a0ad72909d7caa7f5b7c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jsc2rbb3/wheels/b7/24/83/d30234cc013cff538805b14df916e79091f7cf9ee2c5bf3a64\n",
            "Successfully built warmup-scheduler\n",
            "Installing collected packages: warmup-scheduler\n",
            "Successfully installed warmup-scheduler-0.3.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYtKgry8uqb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ4WM8_Guqb6",
        "colab_type": "text"
      },
      "source": [
        "### Step 1. Construct a CNN model\n",
        "\n",
        "#### Implementation 1-1. Design SimCLRHead class\n",
        "\n",
        "#### Implementation 1-2. Design SimCLRNet class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvfdvwkQuqb6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from torchvision.models.resnet import conv3x3\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, norm_layer, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        \n",
        "        self.bn1 = norm_layer(inplanes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        \n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x \n",
        "        residual = self.bn1(residual)\n",
        "        residual = self.relu1(residual)\n",
        "        residual = self.conv1(residual)\n",
        "\n",
        "        residual = self.bn2(residual)\n",
        "        residual = self.relu2(residual)\n",
        "        residual = self.conv2(residual)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "        return x + residual\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, nIn, nOut, stride):\n",
        "        super(Downsample, self).__init__()\n",
        "        self.avg = nn.AvgPool2d(stride)\n",
        "        assert nOut % nIn == 0\n",
        "        self.expand_ratio = nOut // nIn\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.avg(x)\n",
        "        return torch.cat([x] + [x.mul(0)] * (self.expand_ratio - 1), 1)\n",
        "\n",
        "class ResNetCifar(nn.Module):\n",
        "    def __init__(self, depth, width=1, classes=10, channels=3, norm_layer=nn.BatchNorm2d):\n",
        "        assert (depth - 2) % 6 == 0         # depth is 6N+2\n",
        "        self.N = (depth - 2) // 6\n",
        "        super(ResNetCifar, self).__init__()\n",
        "\n",
        "        # Following the Wide ResNet convention, we fix the very first convolution\n",
        "        self.conv1 = nn.Conv2d(channels, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.inplanes = 16\n",
        "        self.layer1 = self._make_layer(norm_layer, 16 * width)\n",
        "        self.layer2 = self._make_layer(norm_layer, 32 * width, stride=2)\n",
        "        self.layer3 = self._make_layer(norm_layer, 64 * width, stride=2)\n",
        "        self.bn = norm_layer(64 * width)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "\n",
        "        # Initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                \n",
        "    def _make_layer(self, norm_layer, planes, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            downsample = Downsample(self.inplanes, planes, stride)\n",
        "        layers = [BasicBlock(self.inplanes, planes, norm_layer, stride, downsample)]\n",
        "        self.inplanes = planes\n",
        "        for i in range(self.N - 1):\n",
        "            layers.append(BasicBlock(self.inplanes, planes, norm_layer))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    \n",
        "class Normalize(nn.Module):\n",
        "\n",
        "    def __init__(self, power=2):\n",
        "        super(Normalize, self).__init__()\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, x):\n",
        "        norm = x.pow(self.power).sum(1, keepdim=True).pow(1. / self.power)\n",
        "        out = x.div(norm)\n",
        "        return out\n",
        "    \n",
        "\n",
        "class SimCLRHead(nn.Module):\n",
        "    def __init__(self, width, emb_dim):\n",
        "        super(SimCLRHead, self).__init__()\n",
        "        \n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        ### 1. Linear layer (64 * width -> 64 * width)\n",
        "        ### 2. ReLU\n",
        "        ### 3. Linear layer (64 * width -> emb_dim)\n",
        "        ### 4. Normalization layer (Normalize module above)\n",
        "        self.fc1 = nn.Linear(64 * width, 64 * width)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc2 = nn.Linear(64 * width, 64 * width)\n",
        "        self.norm = Normalize()\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        ### IMPLEMENTATION 1-1 ###\n",
        "        ### Design a proper forward function\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        return x\n",
        "    \n",
        "\n",
        "class SimCLRNet(nn.Module):\n",
        "    def __init__(self, depth, width=1, num_classes=10, emb_dim=32):\n",
        "        super(SimCLRNet, self).__init__()\n",
        "        \n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "        self.feat = ResNetCifar(depth=depth, width=width, classes=num_classes)\n",
        "        \n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        ### 1. A projection head (SimCLRHead module above)\n",
        "        self.head = SimCLRHead(width, emb_dim)\n",
        "        \n",
        "        ### 2. A linear classifier (64 * width -> num_classes)\n",
        "        self.classifier = nn.Linear(64 * width, num_classes)\n",
        "        \n",
        "        ### 3. Normalization layer for conv feature normalization (Normalize module above)\n",
        "        self.norm = Normalize()\n",
        "        \n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "    \n",
        "    def forward(self, x, norm_feat=False):\n",
        "        \n",
        "        ### IMPLEMENTATION 1-2 ###\n",
        "        ### Your module must return\n",
        "        ### 1. Conv feature (feat) - when norm_feat is true, apply L2 normalization\n",
        "        ### 2. Projected embedding (emb)\n",
        "        ### 3. Logit vector by the linear classifier (logit)\n",
        "        \n",
        "        feat = self.feat(x)\n",
        "        if norm_feat :\n",
        "          feat = self.norm(x)\n",
        "        \n",
        "        emb = self.head(feat)\n",
        "        logit = self.classifier(feat)\n",
        "\n",
        "        ### IMPLEMENTATION ENDS HERE ###\n",
        "        return feat, emb, logit"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvxUSFm-uqb9",
        "colab_type": "text"
      },
      "source": [
        "### Step 2. Prepare datasets & data augmentations\n",
        "\n",
        "For contrastaive learning, a set of random augmentation functions is first defined.\n",
        "\n",
        "Then, the set is applied twice to each image, which is implemented as provided DuplicatedCompose module\n",
        "\n",
        "#### Implementation 2-1. Design a train transform (train_transform)\n",
        "\n",
        "Follow the instruction inside the train_transform\n",
        "\n",
        "https://pytorch.org/docs/stable/torchvision/transforms.html\n",
        "\n",
        "Refer to the torchvision.transforms documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S53vsB4Iuqb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DuplicatedCompose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img1 = img.copy()\n",
        "        img2 = img.copy()\n",
        "        for t in self.transforms:\n",
        "            img1 = t(img1)\n",
        "            img2 = t(img2)\n",
        "        return img1, img2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex83YAnMuqcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "cv2.setNumThreads(0)\n",
        "\n",
        "class GaussianBlur(object):\n",
        "    # Implements Gaussian blur as described in the SimCLR paper\n",
        "    def __init__(self, kernel_size, min=0.1, max=2.0):\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "        # kernel size is set to be 10% of the image height/width\n",
        "        self.kernel_size = kernel_size\n",
        "        \n",
        "        if self.kernel_size % 2 == 0:\n",
        "            self.kernel_size += 1\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        sample = np.array(sample)\n",
        "\n",
        "        # blur the image with a 50% chance\n",
        "        prob = np.random.random_sample()\n",
        "\n",
        "        if prob < 0.5:\n",
        "            sigma = (self.max - self.min) * np.random.random_sample() + self.min\n",
        "            sample = cv2.GaussianBlur(sample, (self.kernel_size, self.kernel_size), sigma)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xHmdyuguqcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "img_size = (32, 32)\n",
        "\n",
        "color_jitter = transforms.ColorJitter(0.8, 0.8, 0.8, 0.2)\n",
        "\n",
        "train_transform = DuplicatedCompose([\n",
        "    ### IMPLEMENTATION 2-1 ###\n",
        "    ### 1. Random resized crop w/ final size of (32, 32)\n",
        "    ### 2. Random horizontal flip w/ p=0.5\n",
        "    ### 3. Randomly apply the pre-defined color jittering w/ p=0.8\n",
        "    ### 4. Random gray scale w/ p=0.2\n",
        "    ### 5. Gaussian blur w/ kernel size of 1/10 of the image width or height (32)\n",
        "    transforms.RandomCrop(img_size),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    color_jitter,\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    GaussianBlur(img_size[0]//10),\n",
        "    \n",
        "    ### IMPLEMENTATION ENDS HERE ###\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo9Xj3KruqcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "62eedddb-ba5d-472b-d17c-1764d9636efd"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='.',\n",
        "                                 train=True,\n",
        "                                 download=True,\n",
        "                                 transform=train_transform\n",
        "                                )\n",
        "\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_size=256,\n",
        "                          num_workers=4,\n",
        "                          shuffle=True,\n",
        "                          drop_last=True\n",
        "                         )"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xwkINzBuqcH",
        "colab_type": "text"
      },
      "source": [
        "### Step 3. Implement InfoNCE loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUjxzqf6uqcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NTXentLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, batch_size, temperature, use_cosine_similarity):\n",
        "        super(NTXentLoss, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.temperature = temperature\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "        self.mask_samples_from_same_repr = self._get_correlated_mask().type(torch.bool)\n",
        "        self.similarity_function = self._get_similarity_function(use_cosine_similarity)\n",
        "        self.criterion = torch.nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "    def _get_similarity_function(self, use_cosine_similarity):\n",
        "        if use_cosine_similarity:\n",
        "            self._cosine_similarity = torch.nn.CosineSimilarity(dim=-1)\n",
        "            return self._cosine_simililarity\n",
        "        else:\n",
        "            return self._dot_simililarity\n",
        "\n",
        "    def _get_correlated_mask(self):\n",
        "        diag = np.eye(2 * self.batch_size)\n",
        "        l1 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=-self.batch_size)\n",
        "        l2 = np.eye((2 * self.batch_size), 2 * self.batch_size, k=self.batch_size)\n",
        "        mask = torch.from_numpy((diag + l1 + l2))\n",
        "        mask = (1 - mask).type(torch.bool)\n",
        "        return mask.cuda()\n",
        "\n",
        "    @staticmethod\n",
        "    def _dot_simililarity(x, y):\n",
        "        v = torch.tensordot(x.unsqueeze(1), y.T.unsqueeze(0), dims=2)\n",
        "        # x shape: (N, 1, C)\n",
        "        # y shape: (1, C, 2N)\n",
        "        # v shape: (N, 2N)\n",
        "        return v\n",
        "\n",
        "    def _cosine_simililarity(self, x, y):\n",
        "        # x shape: (N, 1, C)\n",
        "        # y shape: (1, 2N, C)\n",
        "        # v shape: (N, 2N)\n",
        "        v = self._cosine_similarity(x.unsqueeze(1), y.unsqueeze(0))\n",
        "        return v\n",
        "\n",
        "    def forward(self, zis, zjs):\n",
        "        representations = torch.cat([zjs, zis], dim=0)\n",
        "\n",
        "        similarity_matrix = self.similarity_function(representations, representations)\n",
        "\n",
        "        # filter out the scores from the positive samples\n",
        "        l_pos = torch.diag(similarity_matrix, self.batch_size)\n",
        "        r_pos = torch.diag(similarity_matrix, -self.batch_size)\n",
        "        positives = torch.cat([l_pos, r_pos]).view(2 * self.batch_size, 1)\n",
        "\n",
        "        negatives = similarity_matrix[self.mask_samples_from_same_repr].view(2 * self.batch_size, -1)\n",
        "\n",
        "        logits = torch.cat((positives, negatives), dim=1)\n",
        "        logits = logits / self.temperature\n",
        "\n",
        "        labels = torch.zeros(2 * self.batch_size).cuda().long()\n",
        "        loss = self.criterion(logits, labels)\n",
        "\n",
        "        return loss / (2 * self.batch_size)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-SGunpIuqcK",
        "colab_type": "text"
      },
      "source": [
        "### Step 4. Run pre-training step\n",
        "\n",
        "#### Implementation 4-1. Complete a basic SimCLR training loop\n",
        "\n",
        "https://github.com/ildoonet/pytorch-gradual-warmup-lr\n",
        "\n",
        "The linear warmup scheduler implementation is from a github in the link above\n",
        "\n",
        "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
        "\n",
        "Refer to this documentation to use lr schedulers integrated in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlL2Yl5AuqcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.optimizer import Optimizer, required\n",
        "\n",
        "class SGD_with_lars(Optimizer):\n",
        "    r\"\"\"Implements stochastic gradient descent (optionally with momentum).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, momentum=0, weight_decay=0, trust_coef=1.): # need to add trust coef\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if momentum < 0.0:\n",
        "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
        "        if weight_decay < 0.0:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "        if trust_coef < 0.0:\n",
        "            raise ValueError(\"Invalid trust_coef value: {}\".format(trust_coef))\n",
        "\n",
        "        defaults = dict(lr=lr, momentum=momentum, weight_decay=weight_decay, trust_coef=trust_coef)\n",
        "\n",
        "        super(SGD_with_lars, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(SGD_with_lars, self).__setstate__(state)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            momentum = group['momentum']\n",
        "            trust_coef = group['trust_coef']\n",
        "            global_lr = group['lr']\n",
        "\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                d_p = p.grad.data\n",
        "\n",
        "                p_norm = torch.norm(p.data, p=2)\n",
        "                d_p_norm = torch.norm(d_p, p=2).add_(momentum, p_norm)\n",
        "                lr = torch.div(p_norm, d_p_norm).mul_(trust_coef)\n",
        "\n",
        "                lr.mul_(global_lr)\n",
        "\n",
        "                if weight_decay != 0:\n",
        "                    d_p.add_(weight_decay, p.data)\n",
        "\n",
        "                d_p.mul_(lr)\n",
        "\n",
        "                if momentum != 0:\n",
        "                    param_state = self.state[p]\n",
        "                    if 'momentum_buffer' not in param_state:\n",
        "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
        "                    else:\n",
        "                        buf = param_state['momentum_buffer']\n",
        "                        buf.mul_(momentum).add_(d_p)\n",
        "                    d_p = buf\n",
        "\n",
        "                p.data.add_(-1, d_p)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fjc3wkcuqcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, loader):\n",
        "    \n",
        "    batch_size=256\n",
        "    temperature=0.07\n",
        "\n",
        "    loss_fn = NTXentLoss(batch_size=batch_size, temperature=temperature, use_cosine_similarity=True)\n",
        "    \n",
        "    ### IMPLEMENTATION 4-2 ###\n",
        "    ### 1. Use SGD_with_lars with\n",
        "    ### lr = 0.1 * batch_size / 256\n",
        "    ### momentum = 0.9\n",
        "    ### weight_decay = 1e-6\n",
        "    optimizer = SGD_with_lars(net.parameters(), lr = 0.1 * batch_size / 256, momentum=0.9, weight_decay=1e-6)\n",
        "    \n",
        "    from warmup_scheduler import GradualWarmupScheduler\n",
        "    ### 2. Use GradualWarmupScheduler with\n",
        "    ### multiplier = 1\n",
        "    ### total_epoch = 1/10 of total epochs\n",
        "    ### after_scheduler = optim.lr_scheduler.CosineAnnealingLR\n",
        "    scheduler = GradualWarmupScheduler(optimizer, multiplier=1, total_epoch=20, \n",
        "                                       after_scheduler=optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=200))\n",
        "    \n",
        "    train_start = time.time()\n",
        "    \n",
        "    loss_hist = []\n",
        "\n",
        "    for epoch in range(1, 200 + 1):\n",
        "        \n",
        "        train_loss = 0\n",
        "        net.train()\n",
        "        \n",
        "        epoch_start = time.time()\n",
        "        for idx, (data, target) in enumerate(loader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            ### 3. data variable contains two augmented images\n",
        "            ### -1. send them to your GPU by calling .cuda()\n",
        "            ### -2. forward each of them to net\n",
        "            ### -3. compute the InfoNCE loss\n",
        "            \n",
        "            # target : labels.\n",
        "\n",
        "            zi, zj = data\n",
        "            feat_i, emb_i, logit_i = net(zi.cuda())\n",
        "            feat_j, emb_j, logit_j = net(zj.cuda())\n",
        "            \n",
        "            loss = loss_fn(emb_i, emb_j)\n",
        "\n",
        "            ### IMPLEMENTATION ENDS HERE ###\n",
        "            \n",
        "            train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        train_loss /= (idx + 1)\n",
        "        loss_hist.append(train_loss) # added by junwon\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_time = time.time() - epoch_start\n",
        "        print(\"Epoch\\t\", epoch, \n",
        "              \"\\tLoss\\t\", train_loss, \n",
        "              \"\\tTime\\t\", epoch_time,\n",
        "             )\n",
        "        \n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print('Finished training. Train time was:', elapsed_train_time)\n",
        "\n",
        "    return loss_hist\n",
        "\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2Bqj--ZuqcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8899020e-1ebd-4c58-9cf3-22adf1baccb4"
      },
      "source": [
        "GPU_NUM = '0'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_NUM\n",
        "\n",
        "net = SimCLRNet(26, 1, 10, 32)\n",
        "\n",
        "net.cuda()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimCLRNet(\n",
              "  (feat): ResNetCifar(\n",
              "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (downsample): Downsample(\n",
              "          (avg): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): BasicBlock(\n",
              "        (downsample): Downsample(\n",
              "          (avg): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
              "        )\n",
              "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (1): BasicBlock(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (2): BasicBlock(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "      (3): BasicBlock(\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu1): ReLU(inplace=True)\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu2): ReLU(inplace=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      )\n",
              "    )\n",
              "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
              "  )\n",
              "  (head): SimCLRHead(\n",
              "    (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
              "    (norm): Normalize()\n",
              "  )\n",
              "  (classifier): Linear(in_features=64, out_features=10, bias=True)\n",
              "  (norm): Normalize()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43yf5jCluqcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "8eb1ce45-29db-4873-deab-a5ea07d807da"
      },
      "source": [
        "net.zero_grad()\n",
        "loss_list = train(net, train_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch\t 1 \tLoss\t 6.387218812795786 \tTime\t 59.926681995391846\n",
            "Epoch\t 2 \tLoss\t 4.00650057945496 \tTime\t 59.734951972961426\n",
            "Epoch\t 3 \tLoss\t 0.43090210939065005 \tTime\t 60.6079318523407\n",
            "Epoch\t 4 \tLoss\t 0.21036440176077378 \tTime\t 59.232654333114624\n",
            "Epoch\t 5 \tLoss\t 0.15465733645818172 \tTime\t 59.43998885154724\n",
            "Epoch\t 6 \tLoss\t 0.11985162607370278 \tTime\t 58.77029728889465\n",
            "Epoch\t 7 \tLoss\t 0.10051411462899966 \tTime\t 59.83280301094055\n",
            "Epoch\t 8 \tLoss\t 0.09566674647040856 \tTime\t 59.20227026939392\n",
            "Epoch\t 9 \tLoss\t 0.07896991160053474 \tTime\t 59.02400994300842\n",
            "Epoch\t 10 \tLoss\t 0.07341968061832281 \tTime\t 59.22950315475464\n",
            "Epoch\t 11 \tLoss\t 0.06937564372634276 \tTime\t 59.082884550094604\n",
            "Epoch\t 12 \tLoss\t 0.0660804576598681 \tTime\t 59.275195598602295\n",
            "Epoch\t 13 \tLoss\t 0.06699300347230373 \tTime\t 60.16993498802185\n",
            "Epoch\t 14 \tLoss\t 0.060417894703837544 \tTime\t 58.88829302787781\n",
            "Epoch\t 15 \tLoss\t 0.055829949103868924 \tTime\t 59.31361794471741\n",
            "Epoch\t 16 \tLoss\t 0.05496092559053348 \tTime\t 59.06645941734314\n",
            "Epoch\t 17 \tLoss\t 0.05313144327165224 \tTime\t 59.93861174583435\n",
            "Epoch\t 18 \tLoss\t 0.05164009169317209 \tTime\t 60.12755799293518\n",
            "Epoch\t 19 \tLoss\t 0.046341971174264564 \tTime\t 59.40132737159729\n",
            "Epoch\t 20 \tLoss\t 0.047587190253230244 \tTime\t 59.38704824447632\n",
            "Epoch\t 21 \tLoss\t 0.045669567661407666 \tTime\t 59.118754386901855\n",
            "Epoch\t 22 \tLoss\t 0.04429687515665323 \tTime\t 59.72051811218262\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8TTC-0U-Xkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(loss_hist, xlabel='Iteration number', ylabel='Loss value') :\n",
        "  plt.plot(loss_hist)\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  plt.show()"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVo5Msnl--6B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "033390d7-2cac-49ec-979a-6305d33df160"
      },
      "source": [
        "plot_loss(loss_list)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-d40d53ab0e66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'loss_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ufE49N8BcxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save ckpt in google drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!ls /content/gdrive/My Drive\n",
        "model_save_name = 'simCLR.pt'\n",
        "path = F\"/content/gdrive/My Drive/{model_save_name}\" \n",
        "torch.save(model.state_dict(), path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CPurz9SuqcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " torch.save(model.state_dict(),\"path?\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}